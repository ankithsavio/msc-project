{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "import jax.numpy as jnp\n",
    "import jax\n",
    "from flax import nnx\n",
    "import orbax.checkpoint as ocp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Linear(nnx.Module):\n",
    "  def __init__(self, din: int, dout: int, *, rngs: nnx.Rngs):\n",
    "    key = rngs.params()\n",
    "    self.w = nnx.Param(jax.random.uniform(key, (din, dout)))\n",
    "    self.b = nnx.Param(jnp.zeros((dout,)))\n",
    "    self.din, self.dout = din, dout\n",
    "\n",
    "  def __call__(self, x: jax.Array):\n",
    "    return x @ self.w + self.b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Array([[1.9137444, 2.068491 ]], dtype=float32)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = Linear(din = 3, dout = 2, rngs=nnx.Rngs(params=0))\n",
    "model(x = jnp.ones((1, 3)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "'''\n",
    "\n",
    "\n",
    "        def conv3x3(in_channels, out_channels, stride=1, \n",
    "            padding=1, bias=True, groups=1):    \n",
    "                \n",
    "        return nn.Conv2d(\n",
    "                in_channels,\n",
    "                out_channels,\n",
    "                kernel_size=3,\n",
    "                stride=stride,\n",
    "                padding=padding,\n",
    "                bias=bias,\n",
    "                groups=groups)\n",
    "\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def conv3x3(in_channels, out_channels, stride = 1, padding = 1, bias = True):\n",
    "    return nnx.Conv(in_features= in_channels, \n",
    "                    out_features= out_channels, \n",
    "                    kernel_size= 3, \n",
    "                    strides= stride, \n",
    "                    padding= padding, \n",
    "                    use_bias= bias,\n",
    "                    rngs= nnx.Rngs(params=jax.random.key(0)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def upconv2x2(in_channels, out_channels, mode='transpose'):\n",
    "    '''\n",
    "    Upsample not implemented, need to find alternative\n",
    "    '''\n",
    "    return nnx.ConvTranspose(in_features= in_channels,\n",
    "                                          out_features= out_channels,\n",
    "                                          kernel_size= 2,\n",
    "                                          strides= 2,\n",
    "                                          rngs= nnx.Rngs(params=jax.random.key(0)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def conv1x1(in_channels, out_channels):\n",
    "    return nnx.Conv(in_features= in_channels, \n",
    "                    out_features= out_channels, \n",
    "                    kernel_size= 1, \n",
    "                    strides= 1,\n",
    "                    rngs= nnx.Rngs(params=jax.random.key(0)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Array([[-0.12897272,  0.27571398,  1.1490479 ]], dtype=float32)"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = jnp.ones((1, 3))\n",
    "conv3x3(3, 3)(x)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nnx.relu()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DownConv(nnx.Module):\n",
    " \n",
    "    def __init__(self, in_channels, out_channels, pooling = True):\n",
    "\n",
    "        self.in_channels = in_channels\n",
    "        self.out_channels = out_channels\n",
    "        self.pooling = pooling\n",
    "\n",
    "        self.conv1 = conv3x3(self.in_channels, self.out_channels)\n",
    "        self.conv2 = conv3x3(self.out_channels, self.out_channels)\n",
    "        self.conv3 = conv3x3(self.out_channels, self.out_channels)\n",
    "\n",
    "        # if self.pooling:\n",
    "        #     self.pool = nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "\n",
    "    def __call__(self, x):\n",
    "        \n",
    "        xskip = self.conv1(x)\n",
    "        x = nnx.relu(self.conv2(xskip))\n",
    "        x = nnx.relu(self.conv3(x) + xskip)\n",
    "        # x = F.dropout(x)\n",
    "        before_pool = x\n",
    "        x = jax.lax.cond(self.pooling, \n",
    "                         lambda x : nnx.max_pool(x, window_shape= (2, 2), strides= (2, 2)),\n",
    "                         lambda x : x,\n",
    "                         x)\n",
    "        # if self.pooling:\n",
    "        #     x = nnx.max_pool(x, window_shape= (2, 2), strides= (2, 2))\n",
    "        return x, before_pool\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "class UpConv(nnx.Module):\n",
    "   \n",
    "    def __init__(self, in_channels, out_channels, \n",
    "                 merge_mode='concat', up_mode='transpose'):\n",
    "\n",
    "        self.in_channels = in_channels\n",
    "        self.out_channels = out_channels\n",
    "        self.merge_mode = merge_mode\n",
    "        self.up_mode = up_mode\n",
    "\n",
    "        self.upconv = upconv2x2(self.in_channels, self.out_channels, \n",
    "            mode=self.up_mode)\n",
    "\n",
    "        self.conv1 = jax.lax.cond(self.merge_mode == 'concat',\n",
    "                                  conv3x3(2*self.out_channels, self.out_channels),\n",
    "                                  conv3x3(self.out_channels, self.out_channels))\n",
    "        \n",
    "        # if self.merge_mode == 'concat':\n",
    "        #     self.conv1 = conv3x3(2*self.out_channels, self.out_channels)\n",
    "        # else:\n",
    "        #     self.conv1 = conv3x3(self.out_channels, self.out_channels)\n",
    "\n",
    "        self.conv2 = conv3x3(self.out_channels, self.out_channels)\n",
    "        self.conv3 = conv3x3(self.out_channels, self.out_channels)\n",
    "\n",
    "\n",
    "    def __call__(self, from_down, from_up):\n",
    "        \n",
    "        from_up = self.upconv(from_up)\n",
    "        x = jax.lax.cond(self.merge_mode == 'concat',\n",
    "                         lambda _ : jnp.concatenate([from_up, from_down], axis = 1),\n",
    "                         lambda _ : from_up + from_down)\n",
    "        \n",
    "        # if self.merge_mode == 'concat':\n",
    "        #     x = jnp.concatenate([from_up, from_down], axis = 1) # check axis channel is last for jax\n",
    "        # else:\n",
    "        #     x = from_up + from_down\n",
    "\n",
    "        xskip = self.conv1(x)\n",
    "        x = nnx.relu(self.conv2(xskip))\n",
    "        x = nnx.relu(self.conv3(x) + xskip)\n",
    "        # x = F.dropout(x)\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "true_fun and false_fun output must have identical types, got\nDIFFERENT ShapedArray(float32[0,1,3]) vs. ShapedArray(float32[1,3,3]).",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[50], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m model \u001b[38;5;241m=\u001b[39m DownConv(\u001b[38;5;241m3\u001b[39m, \u001b[38;5;241m3\u001b[39m)\n\u001b[0;32m----> 2\u001b[0m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mjnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mones\u001b[49m\u001b[43m(\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[38;5;241;43m3\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m3\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[49], line 23\u001b[0m, in \u001b[0;36mDownConv.__call__\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     21\u001b[0m \u001b[38;5;66;03m# x = F.dropout(x)\u001b[39;00m\n\u001b[1;32m     22\u001b[0m before_pool \u001b[38;5;241m=\u001b[39m x\n\u001b[0;32m---> 23\u001b[0m x \u001b[38;5;241m=\u001b[39m \u001b[43mjax\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlax\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcond\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpooling\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m     24\u001b[0m \u001b[43m                 \u001b[49m\u001b[38;5;28;43;01mlambda\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mx\u001b[49m\u001b[43m \u001b[49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mnnx\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmax_pool\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mwindow_shape\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m2\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m2\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstrides\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m2\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m2\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     25\u001b[0m \u001b[43m                 \u001b[49m\u001b[38;5;28;43;01mlambda\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mx\u001b[49m\u001b[43m \u001b[49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     26\u001b[0m \u001b[43m                 \u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     27\u001b[0m \u001b[38;5;66;03m# if self.pooling:\u001b[39;00m\n\u001b[1;32m     28\u001b[0m \u001b[38;5;66;03m#     x = nnx.max_pool(x, window_shape= (2, 2), strides= (2, 2))\u001b[39;00m\n\u001b[1;32m     29\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m x, before_pool\n",
      "    \u001b[0;31m[... skipping hidden 3 frame]\u001b[0m\n",
      "File \u001b[0;32m~/anaconda3/envs/rsenv/lib/python3.10/site-packages/jax/_src/lax/control_flow/common.py:215\u001b[0m, in \u001b[0;36m_check_tree_and_avals\u001b[0;34m(what, tree1, avals1, tree2, avals2)\u001b[0m\n\u001b[1;32m    212\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mall\u001b[39m(\u001b[38;5;28mmap\u001b[39m(core\u001b[38;5;241m.\u001b[39mtypematch, avals1, avals2)):\n\u001b[1;32m    213\u001b[0m   diff \u001b[38;5;241m=\u001b[39m tree_map(_show_diff, tree_unflatten(tree1, avals1),\n\u001b[1;32m    214\u001b[0m                   tree_unflatten(tree2, avals2))\n\u001b[0;32m--> 215\u001b[0m   \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mwhat\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m must have identical types, got\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00mdiff\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[0;31mTypeError\u001b[0m: true_fun and false_fun output must have identical types, got\nDIFFERENT ShapedArray(float32[0,1,3]) vs. ShapedArray(float32[1,3,3])."
     ]
    }
   ],
   "source": [
    "model = DownConv(3, 3)\n",
    "model(jnp.ones((1,3, 3)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "Conv.__call__() missing 1 required positional argument: 'inputs'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[45], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m model \u001b[38;5;241m=\u001b[39m \u001b[43mUpConv\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m3\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m3\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m      2\u001b[0m model(jnp\u001b[38;5;241m.\u001b[39mones((\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m3\u001b[39m)), jnp\u001b[38;5;241m.\u001b[39mones((\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m3\u001b[39m)))\n",
      "File \u001b[0;32m~/anaconda3/envs/rsenv/lib/python3.10/site-packages/flax/nnx/nnx/object.py:79\u001b[0m, in \u001b[0;36mObjectMeta.__call__\u001b[0;34m(cls, *args, **kwargs)\u001b[0m\n\u001b[1;32m     78\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__call__\u001b[39m(\u001b[38;5;28mcls\u001b[39m, \u001b[38;5;241m*\u001b[39margs: Any, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs: Any) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Any:\n\u001b[0;32m---> 79\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_graph_node_meta_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mcls\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/rsenv/lib/python3.10/site-packages/flax/nnx/nnx/object.py:88\u001b[0m, in \u001b[0;36m_graph_node_meta_call\u001b[0;34m(cls, *args, **kwargs)\u001b[0m\n\u001b[1;32m     86\u001b[0m node \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__new__\u001b[39m(\u001b[38;5;28mcls\u001b[39m, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m     87\u001b[0m \u001b[38;5;28mvars\u001b[39m(node)[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m_object__state\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m ObjectState()\n\u001b[0;32m---> 88\u001b[0m \u001b[38;5;28;43mcls\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_object_meta_construct\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnode\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     90\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m node\n",
      "File \u001b[0;32m~/anaconda3/envs/rsenv/lib/python3.10/site-packages/flax/nnx/nnx/object.py:82\u001b[0m, in \u001b[0;36mObjectMeta._object_meta_construct\u001b[0;34m(cls, self, *args, **kwargs)\u001b[0m\n\u001b[1;32m     81\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_object_meta_construct\u001b[39m(\u001b[38;5;28mcls\u001b[39m, \u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[0;32m---> 82\u001b[0m   \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;21;43m__init__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[43], line 14\u001b[0m, in \u001b[0;36mUpConv.__init__\u001b[0;34m(self, in_channels, out_channels, merge_mode, up_mode)\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mup_mode \u001b[38;5;241m=\u001b[39m up_mode\n\u001b[1;32m     11\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mupconv \u001b[38;5;241m=\u001b[39m upconv2x2(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39min_channels, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mout_channels, \n\u001b[1;32m     12\u001b[0m     mode\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mup_mode)\n\u001b[0;32m---> 14\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconv1 \u001b[38;5;241m=\u001b[39m \u001b[43mjax\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlax\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcond\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmerge_mode\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m==\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mconcat\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     15\u001b[0m \u001b[43m                          \u001b[49m\u001b[43mconv3x3\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m2\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mout_channels\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mout_channels\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     16\u001b[0m \u001b[43m                          \u001b[49m\u001b[43mconv3x3\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mout_channels\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mout_channels\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     18\u001b[0m \u001b[38;5;66;03m# if self.merge_mode == 'concat':\u001b[39;00m\n\u001b[1;32m     19\u001b[0m \u001b[38;5;66;03m#     self.conv1 = conv3x3(2*self.out_channels, self.out_channels)\u001b[39;00m\n\u001b[1;32m     20\u001b[0m \u001b[38;5;66;03m# else:\u001b[39;00m\n\u001b[1;32m     21\u001b[0m \u001b[38;5;66;03m#     self.conv1 = conv3x3(self.out_channels, self.out_channels)\u001b[39;00m\n\u001b[1;32m     23\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconv2 \u001b[38;5;241m=\u001b[39m conv3x3(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mout_channels, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mout_channels)\n",
      "    \u001b[0;31m[... skipping hidden 11 frame]\u001b[0m\n",
      "File \u001b[0;32m~/anaconda3/envs/rsenv/lib/python3.10/site-packages/jax/_src/linear_util.py:192\u001b[0m, in \u001b[0;36mWrappedFun.call_wrapped\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    189\u001b[0m gen \u001b[38;5;241m=\u001b[39m gen_static_args \u001b[38;5;241m=\u001b[39m out_store \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    191\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 192\u001b[0m   ans \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mf\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43mdict\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mparams\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    193\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m:\n\u001b[1;32m    194\u001b[0m   \u001b[38;5;66;03m# Some transformations yield from inside context managers, so we have to\u001b[39;00m\n\u001b[1;32m    195\u001b[0m   \u001b[38;5;66;03m# interrupt them before reraising the exception. Otherwise they will only\u001b[39;00m\n\u001b[1;32m    196\u001b[0m   \u001b[38;5;66;03m# get garbage-collected at some later time, running their cleanup tasks\u001b[39;00m\n\u001b[1;32m    197\u001b[0m   \u001b[38;5;66;03m# only after this exception is handled, which can corrupt the global\u001b[39;00m\n\u001b[1;32m    198\u001b[0m   \u001b[38;5;66;03m# state.\u001b[39;00m\n\u001b[1;32m    199\u001b[0m   \u001b[38;5;28;01mwhile\u001b[39;00m stack:\n",
      "\u001b[0;31mTypeError\u001b[0m: Conv.__call__() missing 1 required positional argument: 'inputs'"
     ]
    }
   ],
   "source": [
    "model = UpConv(3, 3)\n",
    "model(jnp.ones((1, 3)), jnp.ones((1, 3)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class UN(nnx.Module):\n",
    "    \"\"\" `UNet` class is based on https://arxiv.org/abs/1505.04597\n",
    "    The U-Net is a convolutional encoder-decoder neural network.\n",
    "    Contextual spatial information (from the decoding,\n",
    "    expansive pathway) about an input tensor is merged with\n",
    "    information representing the localization of details\n",
    "    (from the encoding, compressive pathway).\n",
    "    Modifications to the original paper:\n",
    "    (1) padding is used in 3x3 convolutions to prevent loss\n",
    "        of border pixels\n",
    "    (2) merging outputs does not require cropping due to (1)\n",
    "    (3) residual connections can be used by specifying\n",
    "        UNet(merge_mode='add')\n",
    "    (4) if non-parametric upsampling is used in the decoder\n",
    "        pathway (specified by upmode='upsample'), then an\n",
    "        additional 1x1 2d convolution occurs after upsampling\n",
    "        to reduce channel dimensionality by a factor of 2.\n",
    "        This channel halving happens with the convolution in\n",
    "        the tranpose convolution (specified by upmode='transpose')\n",
    "    \"\"\"\n",
    "    def __init__(self, levels, channels=3, depth=5, \n",
    "                 start_filts=64, up_mode='transpose', \n",
    "                 merge_mode='add'):\n",
    "        \n",
    "        self.up_mode = jax.lax.cond((up_mode == 'transpose') | (up_mode == 'upsample'),\n",
    "                                    lambda _ : up_mode,\n",
    "                                    lambda _ : None)\n",
    "        if self.up_mode is None:\n",
    "            raise ValueError(\"\\\"{}\\\" is not a valid mode for \"\n",
    "                             \"upsampling. Only \\\"transpose\\\" and \"\n",
    "                             \"\\\"upsample\\\" are allowed.\".format(up_mode))\n",
    "\n",
    "        # if up_mode in ('transpose', 'upsample'):\n",
    "        #     self.up_mode = up_mode\n",
    "        # else:\n",
    "        #     raise ValueError(\"\\\"{}\\\" is not a valid mode for \"\n",
    "        #                      \"upsampling. Only \\\"transpose\\\" and \"\n",
    "        #                      \"\\\"upsample\\\" are allowed.\".format(up_mode))\n",
    "\n",
    "        self.merge_mode = jax.lax.cond((merge_mode == 'concat') | (merge_mode == 'add'),\n",
    "                                    lambda _ : merge_mode,\n",
    "                                    lambda _ : None)\n",
    "        if self.merge_mode is None:\n",
    "            raise ValueError(\"\\\"{}\\\" is not a valid mode for\"\n",
    "                             \"merging up and down paths. \"\n",
    "                             \"Only \\\"concat\\\" and \"\n",
    "                             \"\\\"add\\\" are allowed.\".format(up_mode))\n",
    "\n",
    "        # if merge_mode in ('concat', 'add'):\n",
    "        #     self.merge_mode = merge_mode\n",
    "        # else:\n",
    "        #     raise ValueError(\"\\\"{}\\\" is not a valid mode for\"\n",
    "        #                      \"merging up and down paths. \"\n",
    "        #                      \"Only \\\"concat\\\" and \"\n",
    "        #                      \"\\\"add\\\" are allowed.\".format(up_mode))\n",
    "\n",
    "        # NOTE: up_mode 'upsample' is incompatible with merge_mode 'add'\n",
    "        if self.up_mode == 'upsample' and self.merge_mode == 'add':\n",
    "            raise ValueError(\"up_mode \\\"upsample\\\" is incompatible \"\n",
    "                             \"with merge_mode \\\"add\\\" at the moment \"\n",
    "                             \"because it doesn't make sense to use \"\n",
    "                             \"nearest neighbour to reduce \"\n",
    "                             \"depth channels (by half).\")\n",
    "\n",
    "        self.levels = levels\n",
    "        self.channels = channels\n",
    "        self.start_filts = start_filts\n",
    "        self.depth = depth\n",
    "\n",
    "        self.down_convs = []\n",
    "        self.up_convs = []\n",
    "        ## vmap? \n",
    "        # create the encoder pathway and add to a list\n",
    "        for i in range(depth):\n",
    "            ins = self.channels * self.levels if i == 0 else outs\n",
    "            outs = self.start_filts*(2**i)\n",
    "        #    outs = self.start_filts\n",
    "            pooling = True if i < depth-1 else False\n",
    "\n",
    "            down_conv = DownConv(ins, outs, pooling=pooling)\n",
    "            self.down_convs.append(down_conv)\n",
    "\n",
    "        # create the decoder pathway and add to a list\n",
    "        # - careful! decoding only requires depth-1 blocks\n",
    "        for i in range(depth-1):\n",
    "            ins = outs\n",
    "            outs = ins // 2\n",
    "        #    outs = ins\n",
    "            up_conv = UpConv(ins, outs, up_mode=up_mode,\n",
    "                merge_mode=merge_mode)\n",
    "            self.up_convs.append(up_conv)\n",
    "\n",
    "        self.conv_final = conv1x1(outs, self.channels)\n",
    "\n",
    "        # add the list of modules to current module\n",
    "        self.down_convs = nn.ModuleList(self.down_convs)\n",
    "        self.up_convs = nn.ModuleList(self.up_convs)\n",
    "\n",
    "        self.reset_params()\n",
    "\n",
    "    @staticmethod\n",
    "    def weight_init(m):\n",
    "        if isinstance(m, nn.Conv2d):\n",
    "            nnx.initializers.xavier_normal(m.weight)\n",
    "            nnx.initializers.constant(m.bias, 0)\n",
    "\n",
    "\n",
    "    def reset_params(self):\n",
    "        for i, m in enumerate(self.modules()):\n",
    "            self.weight_init(m)\n",
    "\n",
    "\n",
    "    def __call__(self, x):\n",
    "        epsilon = 1\n",
    "        \n",
    "        stack = None\n",
    "        ## use of fourier transform to avoid transform\n",
    "        ## no matter what value it lives in the sine wave\n",
    "        ## the input is represented by sin transformation of different frequencies\n",
    "        factor = 10.0\n",
    "        for i in range (self.levels):\n",
    "            scale = x.clone()*(factor**(-i))\n",
    "            scale = torch.sin(scale)\n",
    "            if stack is None:\n",
    "                stack = scale\n",
    "            else:\n",
    "                stack = torch.cat((stack,scale),1)\n",
    "        \n",
    "        x = stack\n",
    "        \n",
    "        encoder_outs = []\n",
    "         \n",
    "        # encoder pathway, save outputs for merging\n",
    "        for i, module in enumerate(self.down_convs):\n",
    "            x, before_pool = module(x)\n",
    "            encoder_outs.append(before_pool)\n",
    "\n",
    "        for i, module in enumerate(self.up_convs):\n",
    "            before_pool = encoder_outs[-(i+2)]\n",
    "            x = module(before_pool, x)\n",
    "        \n",
    "        x = self.conv_final(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        optimizer = optim.Adam(self.parameters(), lr=1e-4)\n",
    "        scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', factor=0.5, patience=10)\n",
    "        \n",
    "        return {\n",
    "           'optimizer': optimizer,\n",
    "           'lr_scheduler': scheduler, \n",
    "           'monitor': 'val_loss'\n",
    "        }\n",
    "    \n",
    "    \n",
    "    def photonLoss(self,result, target):\n",
    "        expEnergy = torch.exp(result)\n",
    "        perImage =  -torch.mean(result*target, dim =(-1,-2,-3), keepdims = True )\n",
    "        perImage += torch.log(torch.mean(expEnergy, dim =(-1,-2,-3), keepdims = True ))*torch.mean(target, dim =(-1,-2,-3), keepdims = True )\n",
    "        return torch.mean(perImage)\n",
    "    \n",
    "    def MSELoss(self,result, target):\n",
    "        expEnergy = torch.exp(result)\n",
    "        expEnergy /= (torch.mean(expEnergy, dim =(-1,-2,-3), keepdims = True ))\n",
    "        target = target / (torch.mean(target, dim =(-1,-2,-3), keepdims = True ))\n",
    "        return torch.mean((expEnergy-target)**2)\n",
    "    \n",
    "    def training_step(self, batch, batch_idx):\n",
    "        loss = self.photonLoss(self(batch[:,self.channels:,...]),batch[:,:self.channels,...] )\n",
    "        self.log(\"train_loss\", loss)\n",
    "        return loss\n",
    "\n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        loss = self.photonLoss(self(batch[:,self.channels:,...]),batch[:,:self.channels,...] )\n",
    "        self.log(\"val_loss\", loss)\n",
    "\n",
    "    def test_step(self, batch, batch_idx):\n",
    "        loss = self.photonLoss(self(batch[:,self.channels:,...]),batch[:,:self.channels,...] )\n",
    "        self.log(\"test_loss\", loss)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
