{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import jax.numpy as jnp\n",
    "import jax\n",
    "import flax.linen as nn\n",
    "import orbax.checkpoint as ocp\n",
    "from jax import make_jaxpr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def conv3x3(in_channels, out_channels, stride = 1, padding = 1, bias = True):\n",
    "    return nn.Conv(features= out_channels,\n",
    "                   kernel_size= (3, 3), \n",
    "                   strides= (stride, stride),\n",
    "                   padding= padding,\n",
    "                   use_bias= bias,\n",
    "                   kernel_init= nn.initializers.xavier_normal(),\n",
    "                   bias_init= nn.initializers.constant(0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def testit(model, input, key, make = False):\n",
    "    if not isinstance(input, tuple): # handles objects with different no. of inputs UpConv and DownConv\n",
    "        x = (input,)\n",
    "    else:\n",
    "        x = input\n",
    "    key, split_key = jax.random.split(key)\n",
    "    variables = model.init(split_key, *x)\n",
    "    results = model.apply(variables, *x)\n",
    "    print(f\"\\nVariable Shapes : \\n{jax.tree.map(lambda x: x.shape, variables)} \\n\\nResults Shapes: {jax.tree.map(lambda x : x.shape, results)}\")\n",
    "    if make:\n",
    "        print(f\"\\n\\n\\nResults : {results}\")\n",
    "        return make_jaxpr(lambda variables, *x: model.apply(variables, *x))(variables, *x)\n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "key = jax.random.key(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "        key, split_key = jax.random.split(key)\n",
    "        testit(conv3x3(3, 3,), jnp.ones((1, 3, 3, 1)), key)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Variable Shapes : \n",
      "{'params': {'bias': (3,), 'kernel': (3, 3, 1, 3)}} \n",
      "\n",
      "Results Shapes: (1, 3, 3, 3)\n"
     ]
    }
   ],
   "source": [
    "key, split_key = jax.random.split(key)\n",
    "testit(conv3x3(3, 3,), jnp.ones((1, 3, 3, 1)), key)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def conv1x1(in_channels, out_channels):\n",
    "    return nn.Conv(features= out_channels, \n",
    "                   kernel_size= 1,\n",
    "                   strides= 1,\n",
    "                   kernel_init= nn.initializers.xavier_normal(),\n",
    "                   bias_init= nn.initializers.constant(0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(32, 32)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "shape = (1, 32, 32, 1)\n",
    "_, h, w = shape[:-1]\n",
    "h, w"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Upsample(nn.Module):\n",
    "    method : str = 'bilinear'\n",
    "    scale : int = 2\n",
    "\n",
    "    @nn.compact\n",
    "    def __call__(self, x):\n",
    "        shape = x.shape\n",
    "        shape = (shape[0], shape[1]*self.scale, shape[2]*self.scale, shape[3])\n",
    "        return jax.image.resize(image = x, shape = shape, method = self.method)\n",
    "\n",
    "def upconv2x2(in_channels, out_channels, mode='transpose'):\n",
    "    '''\n",
    "    Upsample not implemented, need to find alternative\n",
    "    '''\n",
    "    if mode == 'transpose':\n",
    "        return nn.ConvTranspose(features= out_channels,\n",
    "                            kernel_size= (2, 2),\n",
    "                            strides= (2, 2))\n",
    "    else:\n",
    "        return nn.Sequential([Upsample(method = 'bilinear', scale = 2),\n",
    "                              conv1x1(in_channels, out_channels)])\n",
    "                   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Variable Shapes : \n",
      "{'params': {'bias': (3,), 'kernel': (2, 2, 1, 3)}} \n",
      "\n",
      "Results Shapes: (1, 6, 6, 3)\n"
     ]
    }
   ],
   "source": [
    "key, split_key = jax.random.split(key)\n",
    "testit(upconv2x2(3, 3, mode = 'transpose'), jnp.ones((1, 3, 3, 1)), key)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Variable Shapes : \n",
      "{'params': {'layers_1': {'bias': (3,), 'kernel': (1, 1, 3)}}} \n",
      "\n",
      "Results Shapes: (1, 6, 6, 3)\n"
     ]
    }
   ],
   "source": [
    "key, split_key = jax.random.split(key)\n",
    "testit(upconv2x2(3, 3, mode = 'bilinear'), jnp.ones((1, 3, 3, 1)), key)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "        key, split_key = jax.random.split(key)\n",
    "        testit(conv1x1(3, 3,), jnp.ones((1, 3)), key)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DownConv(nn.Module):\n",
    "    in_channels : int\n",
    "    out_channels : int\n",
    "    pooling : bool\n",
    "\n",
    "    def setup(self):\n",
    "        self.conv1 = conv3x3(self.in_channels, self.out_channels)\n",
    "        self.conv2 = conv3x3(self.out_channels, self.out_channels)\n",
    "        self.conv3 = conv3x3(self.out_channels, self.out_channels)\n",
    "\n",
    "    def __call__(self, x):\n",
    "        \n",
    "        xskip = self.conv1(x)\n",
    "        x = nn.relu(self.conv2(xskip))\n",
    "        x = nn.relu(self.conv3(x) + xskip)\n",
    "        before_pool = x\n",
    "\n",
    "        if self.pooling:\n",
    "            x = nn.max_pool(x,\n",
    "                            window_shape= (2, 2),\n",
    "                            strides= (2, 2))\n",
    "            \n",
    "        return x, before_pool\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "        key, split_key = jax.random.split(key)\n",
    "        testit(DownConv(3, 3, True), jnp.ones((1, 3, 3, 3)), key = key, make = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class UpConv(nn.Module):\n",
    "    in_channels : int\n",
    "    out_channels : int\n",
    "    merge_mode : str = 'concat'\n",
    "    up_mode : str = 'transpose'\n",
    "\n",
    "    def setup(self):\n",
    "        self.upconv = upconv2x2(self.in_channels, self.out_channels,mode=self.up_mode)\n",
    "        self.conv1 = conv3x3(self.out_channels, self.out_channels) ## refine for flax\n",
    "        self.conv2 = conv3x3(self.out_channels, self.out_channels)\n",
    "        self.conv3 = conv3x3(self.out_channels, self.out_channels)\n",
    "\n",
    "    def __call__(self, from_down, from_up):\n",
    "        \n",
    "        from_up = self.upconv(from_up) \n",
    "        if self.merge_mode == 'concat':\n",
    "            x = jnp.concatenate([from_up, from_down], axis = -1) # check axis channel is last for jax\n",
    "        else:\n",
    "            x = from_up + from_down\n",
    "\n",
    "        xskip = self.conv1(x) \n",
    "        x = nn.relu(self.conv2(xskip))\n",
    "        x = nn.relu(self.conv3(x) + xskip)\n",
    "\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Variable Shapes : \n",
      "{'params': {'conv1': {'bias': (128,), 'kernel': (3, 3, 192, 128)}, 'conv2': {'bias': (128,), 'kernel': (3, 3, 128, 128)}, 'conv3': {'bias': (128,), 'kernel': (3, 3, 128, 128)}, 'upconv': {'bias': (128,), 'kernel': (2, 2, 64, 128)}}} \n",
      "\n",
      "Results Shapes: (1, 4, 4, 128)\n"
     ]
    }
   ],
   "source": [
    "key, split_key = jax.random.split(key)\n",
    "a = jnp.ones((1, 4, 4, 64))\n",
    "b = jnp.ones((1, 2, 2, 64))\n",
    "x = (a, b)\n",
    "testit(UpConv(3, 128), input = x, key = key, make = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "        key, split_key = jax.random.split(key)\n",
    "        a = jnp.ones((1, 4, 4, 64))\n",
    "        b = jnp.ones((1, 2, 2, 64))\n",
    "        x = (a, b)\n",
    "        testit(UpConv(3, 128), input = x, key = key, make = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([-0.9589])\n",
      "-0.9589243\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "print(torch.sin(torch.tensor([5])))\n",
    "print(jnp.sin(5))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Add Xavier Init to the model by passing the arguments for each module in the [UN](https://uvadlc-notebooks.readthedocs.io/en/latest/tutorial_notebooks/JAX/tutorial4/Optimization_and_Initialization.html#:~:text=kernel_init%20%3A%20Callable%20%3D%20nn.linear.default_kernel_init)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### [LR Scheduler](https://uvadlc-notebooks.readthedocs.io/en/latest/tutorial_notebooks/JAX/tutorial5/Inception_ResNet_DenseNet.html#:~:text=of%20the%20training-,lr_schedule,-%3D%20optax.) \n",
    "\n",
    "#### ReduceONPlateaur available in optax\n",
    "\n",
    "#### [Clip Gradients](https://uvadlc-notebooks.readthedocs.io/en/latest/tutorial_notebooks/JAX/tutorial6/Transformers_and_MHAttention.html#:~:text=optimizer%20%3D%20optax.chain(%0A%20%20%20%20%20%20%20%20%20%20%20%20optax.clip_by_global_norm(1.0)%2C%20%20%23%20Clip%20gradients%20at%20norm%201%0A%20%20%20%20%20%20%20%20%20%20%20%20optax.adam(lr_schedule)%0A%20%20%20%20%20%20%20%20))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "class UN(nn.Module):\n",
    "    levels : int\n",
    "    channels : int = 3\n",
    "    depth : int = 5\n",
    "    start_filts : int = 64\n",
    "    up_mode : str = 'transpose'\n",
    "    merge_mode : str = 'add'\n",
    "\n",
    "    def setup(self):\n",
    "        if self.up_mode not in ('transpose', 'upsample'):\n",
    "            raise ValueError(\"\\\"{}\\\" is not a valid mode for \"\n",
    "                             \"upsampling. Only \\\"transpose\\\" and \"\n",
    "                             \"\\\"upsample\\\" are allowed.\".format(self.up_mode))\n",
    "        \n",
    "        if self.merge_mode not in ('concat', 'add'):\n",
    "            raise ValueError(\"\\\"{}\\\" is not a valid mode for\"\n",
    "                             \"merging up and down paths. \"\n",
    "                             \"Only \\\"concat\\\" and \"\n",
    "                             \"\\\"add\\\" are allowed.\".format(self.up_mode))\n",
    "\n",
    "        if self.up_mode == 'upsample' and self.merge_mode == 'add':\n",
    "            raise ValueError(\"up_mode \\\"upsample\\\" is incompatible \"\n",
    "                             \"with merge_mode \\\"add\\\" at the moment \"\n",
    "                             \"because it doesn't make sense to use \"\n",
    "                             \"nearest neighbour to reduce \"\n",
    "                             \"depth channels (by half).\")\n",
    "        \n",
    "        down_convs = []\n",
    "        up_convs = []\n",
    "\n",
    "        for i in range(self.depth):\n",
    "            ins = self.channels * self.levels if i == 0 else outs\n",
    "            outs = self.start_filts*(2**i)\n",
    "            pooling = True if i < self.depth-1 else False\n",
    "\n",
    "            module = DownConv(ins, outs, pooling=pooling)\n",
    "            down_convs.append(module)\n",
    "        self.down_convs = down_convs\n",
    "\n",
    "        for i in range(self.depth-1):\n",
    "            ins = outs\n",
    "            outs = ins // 2\n",
    "            module = UpConv(ins, outs, up_mode=self.up_mode,merge_mode=self.merge_mode)\n",
    "            up_convs.append(module)\n",
    "        self.up_convs = up_convs\n",
    "\n",
    "        self.conv_final = conv1x1(outs, self.channels)\n",
    "\n",
    "\n",
    "\n",
    "    def __call__(self, x):\n",
    "        \n",
    "        stack = None\n",
    "        factor = 10.0\n",
    "        for i in range (self.levels):\n",
    "            scale = x.copy()*(factor**(-i))\n",
    "            scale = jnp.sin(scale)\n",
    "            if stack is None:\n",
    "                stack = scale\n",
    "            else:\n",
    "                stack = jnp.concatenate([stack,scale],axis = -1)\n",
    "        \n",
    "        x = stack\n",
    "        \n",
    "        encoder_outs = []\n",
    "        for i, module in enumerate(self.down_convs):\n",
    "            x, before_pool = module(x)\n",
    "            encoder_outs.append(before_pool)\n",
    "\n",
    "        for i, module in enumerate(self.up_convs):\n",
    "            before_pool = encoder_outs[-(i+2)]\n",
    "            x = module(before_pool, x)\n",
    "        \n",
    "        x = self.conv_final(x)\n",
    "\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "        key, split_key = jax.random.split(key)\n",
    "        a = jnp.ones((1, 128, 128, 3))\n",
    "        # b = jnp.ones((1, 3, 3, 64))\n",
    "        x = a\n",
    "        testit(UN(levels= 10), input = x, key = key, make = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Variable Shapes : \n",
      "{'params': {'conv_final': {'bias': (3,), 'kernel': (1, 64, 3)}, 'down_convs_0': {'conv1': {'bias': (64,), 'kernel': (3, 3, 30, 64)}, 'conv2': {'bias': (64,), 'kernel': (3, 3, 64, 64)}, 'conv3': {'bias': (64,), 'kernel': (3, 3, 64, 64)}}, 'down_convs_1': {'conv1': {'bias': (128,), 'kernel': (3, 3, 64, 128)}, 'conv2': {'bias': (128,), 'kernel': (3, 3, 128, 128)}, 'conv3': {'bias': (128,), 'kernel': (3, 3, 128, 128)}}, 'down_convs_2': {'conv1': {'bias': (256,), 'kernel': (3, 3, 128, 256)}, 'conv2': {'bias': (256,), 'kernel': (3, 3, 256, 256)}, 'conv3': {'bias': (256,), 'kernel': (3, 3, 256, 256)}}, 'down_convs_3': {'conv1': {'bias': (512,), 'kernel': (3, 3, 256, 512)}, 'conv2': {'bias': (512,), 'kernel': (3, 3, 512, 512)}, 'conv3': {'bias': (512,), 'kernel': (3, 3, 512, 512)}}, 'down_convs_4': {'conv1': {'bias': (1024,), 'kernel': (3, 3, 512, 1024)}, 'conv2': {'bias': (1024,), 'kernel': (3, 3, 1024, 1024)}, 'conv3': {'bias': (1024,), 'kernel': (3, 3, 1024, 1024)}}, 'up_convs_0': {'conv1': {'bias': (512,), 'kernel': (3, 3, 512, 512)}, 'conv2': {'bias': (512,), 'kernel': (3, 3, 512, 512)}, 'conv3': {'bias': (512,), 'kernel': (3, 3, 512, 512)}, 'upconv': {'bias': (512,), 'kernel': (2, 2, 1024, 512)}}, 'up_convs_1': {'conv1': {'bias': (256,), 'kernel': (3, 3, 256, 256)}, 'conv2': {'bias': (256,), 'kernel': (3, 3, 256, 256)}, 'conv3': {'bias': (256,), 'kernel': (3, 3, 256, 256)}, 'upconv': {'bias': (256,), 'kernel': (2, 2, 512, 256)}}, 'up_convs_2': {'conv1': {'bias': (128,), 'kernel': (3, 3, 128, 128)}, 'conv2': {'bias': (128,), 'kernel': (3, 3, 128, 128)}, 'conv3': {'bias': (128,), 'kernel': (3, 3, 128, 128)}, 'upconv': {'bias': (128,), 'kernel': (2, 2, 256, 128)}}, 'up_convs_3': {'conv1': {'bias': (64,), 'kernel': (3, 3, 64, 64)}, 'conv2': {'bias': (64,), 'kernel': (3, 3, 64, 64)}, 'conv3': {'bias': (64,), 'kernel': (3, 3, 64, 64)}, 'upconv': {'bias': (64,), 'kernel': (2, 2, 128, 64)}}}} \n",
      "\n",
      "Results Shapes: (1, 128, 128, 3)\n"
     ]
    }
   ],
   "source": [
    "key, split_key = jax.random.split(key)\n",
    "a = jnp.ones((1, 128, 128, 3))\n",
    "# b = jnp.ones((1, 3, 3, 64))\n",
    "x = a\n",
    "testit(UN(levels= 10), input = x, key = key, make = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def photonLoss(result, target):\n",
    "        expEnergy = jnp.exp(result)\n",
    "        perImage = -jnp.mean(result*target, axis = (-1, -2, -3), keepdims=True)\n",
    "        perImage += jnp.log(jnp.mean(expEnergy, axis = (-1, -2, -3), keepdims= True))*jnp.mean(target, axis = (-1, -2, -3), keepdims= True)\n",
    "        return jnp.mean(perImage)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.chdir('/mnt/c/Users/ankit/Desktop/Msc AIML/msc-project/GAP/gap')\n",
    "from BinomDatasetv4 import BinomDataset\n",
    "from torch.utils.data import DataLoader\n",
    "from tifffile import imread, imsave\n",
    "import numpy as np\n",
    "# from BinomDataset_JAX import BinomDataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def collate_fn(batch):\n",
    "    transposed_data = list(zip(*batch))\n",
    "    labels = np.array(transposed_data[1])\n",
    "    imgs = np.stack(transposed_data[0])\n",
    "    return imgs, labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "data = np.concatenate((imread('/mnt/c/Users/ankit/Desktop/Msc AIML/Data/convallaria/trainingDataGT.tif'), imread('/mnt/c/Users/ankit/Desktop/Msc AIML/Data/convallaria/testDataGT.tif')))\n",
    "minpsnr = -40\n",
    "maxpsnr = -5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = BinomDataset(data[:round(data.shape[0]*0.9)], 256, minpsnr, maxpsnr)\n",
    "val_dataset = BinomDataset(data[round(data.shape[0]*0.9):], 256, minpsnr, maxpsnr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = DataLoader(dataset, batch_size= 1, shuffle = True, drop_last= True, pin_memory= False, num_workers= 12, collate_fn= collate_fn)\n",
    "val_loader = DataLoader(val_dataset, batch_size= 1, shuffle= False, drop_last= False, pin_memory= False, num_workers= 12, collate_fn= collate_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ankith/anaconda3/envs/rsenv/lib/python3.10/multiprocessing/popen_fork.py:66: RuntimeWarning: os.fork() was called. os.fork() is incompatible with multithreaded code, and JAX is multithreaded, so this will likely lead to a deadlock.\n",
      "  self.pid = os.fork()\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ankith/anaconda3/envs/rsenv/lib/python3.10/multiprocessing/popen_fork.py:66: RuntimeWarning: os.fork() was called. os.fork() is incompatible with multithreaded code, and JAX is multithreaded, so this will likely lead to a deadlock.\n",
      "  self.pid = os.fork()\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 97,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(next(iter(train_loader)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## the PhotonLoss can be implemented directly in python + jax as the optax follows the same method"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Valid Training Loop for GAP\n",
    "* [Flax Training with TrainState](https://uvadlc-notebooks.readthedocs.io/en/latest/tutorial_notebooks/JAX/tutorial3/Activation_Functions.html#Training-a-model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TrainerModule:\n",
    "    def __init__(self, root_dir, gradient_clip_val, epochs, dataloader, lr = 1e-4, seed = 42):\n",
    "        super().__init__()\n",
    "        self.root_dir = root_dir\n",
    "        self.gradient_clip_val = gradient_clip_val\n",
    "        self.max_epochs = epochs\n",
    "        self.img = next(iter(dataloader))\n",
    "        self.learning_rate= lr\n",
    "        self.seed = seed\n",
    "        self.init_training()\n",
    "        self.init_model()\n",
    "    \n",
    "    def init_training(self):\n",
    "        def train_step(state, rng, batch):\n",
    "\n",
    "        \n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
